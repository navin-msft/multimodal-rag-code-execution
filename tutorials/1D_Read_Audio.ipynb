{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Audio Files\n",
    "\n",
    "Read WAV or MP3 Files with Python and transcribe to text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-cognitiveservices-speech in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (1.42.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (1.64.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\navinparmar\\source\\repos\\multimodal-rag-code-execution\\.conda\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-cognitiveservices-speech\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speech_sdk\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "sys.path.append('..\\\\code')\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from PIL import Image\n",
    "from doc_utils import *\n",
    "from utils.bcolors import bcolors as bc  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure we have the Azure Speech information\n",
    "\n",
    "We will need the Speech APIKEY, REGION and LANGUAGE for this notebook.\n",
    "\n",
    "When running the below cell, the values should reflect the Azure Speech reource you have created in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_info = {\n",
    "        'SPEECH_APIKEY': os.environ.get('SPEECH_APIKEY'),\n",
    "        'SPEECH_REGION': os.environ.get('SPEECH_REGION'),\n",
    "        'SPEECH_LANGUAGE': os.environ.get('SPEECH_LANGUAGE'),\n",
    "}\n",
    "\n",
    "speech_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "        'AZURE_OPENAI_MODEL_WHISPER': os.environ.get('AZURE_OPENAI_MODEL_WHISPER'),\n",
    "        'AZURE_OPENAI_KEY': os.environ.get('AZURE_OPENAI_KEY'),\n",
    "        'AZURE_OPENAI_MODEL_WHISPER': os.environ.get('AZURE_OPENAI_MODEL_WHISPER'),\n",
    "        'AZURE_OPENAI_ENDPOINT_WHISPER': os.environ.get('AZURE_OPENAI_ENDPOINT_WHISPER'),\n",
    "        'AZURE_OPENAI_VERSION_WHISPER': os.environ.get('AZURE_OPENAI_VERSION_WHISPER'),\n",
    "        'AZURE_OPENAI_RESOURCE': os.environ.get('AZURE_OPENAI_RESOURCE'),\n",
    "        'AZURE_OPENAI_MODEL_VISION': os.environ.get('AZURE_OPENAI_MODEL_VISION'),\n",
    "        'AZURE_OPENAI_MODEL': os.environ.get('AZURE_OPENAI_MODEL'),\n",
    "}\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Definitions\n",
    "\n",
    "Defining the functions that will read in the audio file and return the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Azure Speech Service\n",
    "def config_speech_service():\n",
    "    try:\n",
    "        speech_config = speech_sdk.SpeechConfig(\n",
    "            subscription=speech_info['SPEECH_APIKEY'], \n",
    "            region=speech_info['SPEECH_REGION'], \n",
    "            speech_recognition_language=speech_info['SPEECH_LANGUAGE'])\n",
    "\n",
    "        # Set parameters\n",
    "        speech_config.set_property(speech_sdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, \"5000\")\n",
    "        speech_config.set_property(speech_sdk.PropertyId.Speech_SegmentationSilenceTimeoutMs, \"2000\")\n",
    "        speech_config.set_property(speech_sdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, \"5000\")\n",
    "    \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "    return speech_config\n",
    "\n",
    "# Execute the transcription from file with Azure Speech service \n",
    "def speech_recognize_continuous_from_file(speech_config, filename):\n",
    "    # Performs continuous speech recognition with input from an audio file\"\"\"\n",
    "    audio_config = speech_sdk.AudioConfig(filename=filename)\n",
    "\n",
    "    speech_recognizer = speech_sdk.SpeechRecognizer(speech_config, audio_config)\n",
    "\n",
    "    done = False\n",
    "    transcription = []\n",
    "\n",
    "    # Callback that signals to stop continuous recognition upon receiving an event `evt`\n",
    "    def stop_cb(evt: speech_sdk.SessionEventArgs):\n",
    "        print('CLOSING')\n",
    "        nonlocal done\n",
    "        done = True\n",
    "    \n",
    "    # Callback that signals the recognition has been canceled\n",
    "    def speech_recognizer_recognition_canceled_cb(evt: speech_sdk.SessionEventArgs):\n",
    "        print('Canceled event')\n",
    "\n",
    "    # Callback that signals the recognition session has been stopped\n",
    "    def speech_recognizer_session_stopped_cb(evt: speech_sdk.SessionEventArgs):\n",
    "        print('SessionStopped event')\n",
    "\n",
    "    # Callback while transcribing\n",
    "    def speech_recognizer_recognizing_cb(evt: speech_sdk.SpeechRecognitionEventArgs):\n",
    "        print('Transcribing: ', evt.result.text)\n",
    "\n",
    "    # Callback when a sentence has finished\n",
    "    def speech_recognizer_transcribed_cb(evt: speech_sdk.SpeechRecognitionEventArgs):\n",
    "        print('TRANSCRIBED:')\n",
    "        if evt.result.reason == speech_sdk.ResultReason.RecognizedSpeech:\n",
    "            print(f'\\tText: {evt.result.text}')\n",
    "            transcription.append(evt.result.text)\n",
    "        elif evt.result.reason == speech_sdk.ResultReason.NoMatch:\n",
    "            print(f'\\tNOMATCH: Speech could not be TRANSCRIBED: {evt.result.no_match_details}')\n",
    "            stop_cb(evt)\n",
    "\n",
    "    # Callback that signal the session has started\n",
    "    def speech_recognizer_session_started_cb(evt: speech_sdk.SessionEventArgs):\n",
    "        print('SessionStarted event')\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer\n",
    "    speech_recognizer.recognizing.connect(speech_recognizer_recognizing_cb)\n",
    "    speech_recognizer.recognized.connect(speech_recognizer_transcribed_cb)\n",
    "    speech_recognizer.session_started.connect(speech_recognizer_session_started_cb)\n",
    "    speech_recognizer.session_stopped.connect(speech_recognizer_session_stopped_cb)\n",
    "    speech_recognizer.canceled.connect(speech_recognizer_recognition_canceled_cb)\n",
    "    # stop transcribing on either session stopped or canceled events\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous speech recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "            \n",
    "    final_text = \"\"\n",
    "    for text in transcription:\n",
    "        final_text += text + \" \\n\"\n",
    "    #print(f'TRANSCRIPTION: [{final_text}]')\n",
    "\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "    return transcription\n",
    "\n",
    "def config_whisper():\n",
    "    whisper_client = AzureOpenAI(\n",
    "        api_key=model_info['AZURE_OPENAI_KEY'],  \n",
    "        api_version=model_info['AZURE_OPENAI_VERSION_WHISPER'],\n",
    "        base_url=f\"{model_info['AZURE_OPENAI_ENDPOINT_WHISPER']}/openai/deployments/{model_info['AZURE_OPENAI_MODEL_WHISPER']}\"\n",
    "    )\n",
    "\n",
    "    return whisper_client\n",
    "\n",
    "\n",
    "def transcribe_with_whisper(whisper_client, filename):\n",
    "    try:\n",
    "        transcript = whisper_client.audio.transcriptions.create(\n",
    "            file=open(filename, \"rb\"), \n",
    "            model=model_info['AZURE_OPENAI_MODEL_WHISPER']\n",
    "            )\n",
    "        return transcript\n",
    "    \n",
    "    except Exception as ex:\n",
    "        return ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Audio File\n",
    "\n",
    "Read the audio file and print the transcription out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribing using Speech Services (AI Service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage with Azure Speech service\n",
    "speech_config=config_speech_service()\n",
    "#file_path = 'sample_data/sample_audio_parte_accidente.wav'\n",
    "#file_path = 'sample_data/The_National_Park.wav'\n",
    "#file_path = 'sample_data/CNVSample049.wav'\n",
    "file_path = 'sample_data/call_recording_en.wav'\n",
    "transcript = speech_recognize_continuous_from_file(speech_config, file_path)\n",
    "display(transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage with Azure Speech service\n",
    "speech_config=config_speech_service()\n",
    "#file_path = 'sample_data/sample_audio_parte_accidente.wav'\n",
    "#file_path = 'sample_data/The_National_Park.wav'\n",
    "#file_path = 'sample_data/CNVSample049.wav'\n",
    "file_path = 'sample_data/sample_audio_parte_accidente.wav'\n",
    "transcript = speech_recognize_continuous_from_file(speech_config, file_path)\n",
    "display(transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribing using Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transcription(text='Hola, acabo de tener un accidente y quería notificarlo. Hola, de acuerdo. Espero que esté bien. ¿Qué ha pasado? Estaba conduciendo por la carretera de Colmenar y me he dado un golpe con otro coche. ¿Está usted bien? Sí, solo un poco nervioso. Es normal. ¿Me puede decir su nombre completo? Claro, me llamo Álvaro Gómez Rodríguez. ¿Sabe cuál ha sido la causa del accidente? Creo que he golpeado un bache. De acuerdo. ¿Dónde se ha producido el accidente? En la carretera de Colmenar, pasada la salida 17. ¿Ha habido algún otro herido? Creo que no, pero no estoy seguro. De acuerdo, lo investigaremos. ¿Me puede dar la información del otro conductor? Sí, su nombre es Juan Delgado Rivera. De acuerdo, un momento, por favor. ¿Me puede decir su DNI, por favor? Sí. Es 12345678F. De acuerdo. ¿Qué daños ha sufrido el coche? Se ha roto el faro delantero derecho y se ha pinchado una rueda. ¿Puede conducir el coche? No lo sé, no. Va a venir a recogerlo la grúa. De acuerdo, necesitaremos inspeccionar el coche. Voy a dar de alta el parte para proceder la petición de inspección y reparación. Perfecto, muchas gracias por su ayuda.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage with Whisper\n",
    "whisper_client = config_whisper()\n",
    "#file_path = 'sample_data/CNVSample049.wav'\n",
    "file_path = 'sample_data/sample_audio_parte_accidente.wav'\n",
    "transcript2 = transcribe_with_whisper(whisper_client, file_path)\n",
    "display(transcript2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transcription(text=\"Good afternoon and thank you for calling Contoso Suites. This call is being recorded for quality assurance purposes. My name is Cameron Baker. How can I assist you today? Uh, yes, hi. My name is Parker McLean and I'm calling because I wanted to change some of the details on my upcoming stay. Thank you for choosing Contoso Suites, Mr. McLean. Could you please tell me what hotel your reservation is for so I can look up the details of your stay? Uh, I'm staying at the, uh, the Airport Gateway Hotel. Okay, I found your reservation. Can you please confirm your check-in and check-out dates for me? I check in on, uh, on the 9th and check out on... Hang on, let me check. Um, right, right, right, okay. Yeah, so I'm checking in on the 9th and out on the 14th. And I requested two meeting rooms for the 11th and the 12th. Thank you. That matches what I'm seeing on your reservation. What changes were you looking to make today? I'm hoping to change the date from that hotel to the, uh, to the Downtown Plaza Hotel instead. So, um, it'd be a change to my reservation and, and the two meeting rooms. Are you looking to change the dates or will they stay the same? Uh, the dates would be the same. Okay, give me a moment to check if there's availability at the Downtown Plaza Hotel for those dates. Unfortunately, it does not appear the Downtown Plaza Hotel is available for your dates. However, I do have availability for both the room and meeting rooms at the Grand Contoso Downtown Seattle. This hotel is part of Contoso's Luxury Hotels Collection, so it is more expensive, but is only a block away from the Downtown Plaza Hotel, so it will still keep you right in the center of the downtown area. It also features a Michelin Star restaurant, which is one of the best in Seattle. Let me know what the price would be for that switch. The total price for the room, checking in on the 9th and out on the 14th, plus two meeting rooms for the 11th and 12th, comes out to $2,378. That's $1,578 for the room and $800 for the meeting rooms. I also see you're a platinum member of our rewards program. Thank you for your loyalty. For that, I can upgrade you to one of our executive suites. That is a larger room with an excellent view of the Puget Sound. Would you like me to make the change to your upcoming reservation? I'd be grateful. Okay, your reservation has been updated. Thank you. Thank you for choosing Contoso Suites. Have a nice day.\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage with Whisper\n",
    "whisper_client = config_whisper()\n",
    "#file_path = 'sample_data/CNVSample049.wav'\n",
    "file_path = 'sample_data/01_Customer_Service_Call.wav'\n",
    "transcript = transcribe_with_whisper(whisper_client, file_path)\n",
    "display(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import OAI_client\n",
    "#from joblib import Parallel, delayed\n",
    "import time\n",
    "import itertools\n",
    "import pdb\n",
    "\n",
    "\n",
    "def construct_prompt(prompt_parameters):\n",
    "\n",
    "    prompt = \"\"\"\n",
    "\n",
    "    Document Body\n",
    "\n",
    "    Review of {prompt_parameters}\n",
    "\n",
    "\n",
    "    |endoftext|\n",
    "\n",
    "    Write answer for questions only referring to the facts in document above. \n",
    "    Sentiment can either be positive, neutral or negative.\n",
    "\n",
    "    |endoftext|\n",
    "\n",
    "    ### Questions\n",
    "    1) What is the overall sentiment?\n",
    "    2) List the sentiment aspects as bullet points.\n",
    "    \"\"\"\n",
    "\n",
    "    return(prompt)\n",
    "\n",
    "\n",
    "\n",
    "def extract_sentiment_aspects_for_batch(reviews_batch, batch_name):\n",
    "\n",
    "\n",
    "    prompt = \"\"\"\n",
    "\n",
    "    Document Body\n",
    "\n",
    "    Review of {prompt_parameters}\n",
    "\n",
    "\n",
    "    |endoftext|\n",
    "\n",
    "    Write answer for questions only referring to the facts in document above. \n",
    "    Sentiment can either be positive, neutral or negative.\n",
    "\n",
    "    |endoftext|\n",
    "\n",
    "    ### Questions\n",
    "    1) What is the overall sentiment?\n",
    "    2) List the sentiment aspects as bullet points.\n",
    "    3) Overall Sentiment\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    p = prompt.format(prompt_parameters = reviews_batch)\n",
    "    print(p)\n",
    "    output = ask_LLM(p, model_info=model_info)\n",
    "    print(output)\n",
    "\n",
    "    results = output\n",
    "  \n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage with Whisper\n",
    "whisper_client = config_whisper()\n",
    "#file_path = 'sample_data/CNVSample049.wav'\n",
    "file_path = 'sample_data/01_Customer_Service_Call.wav'\n",
    "transcript = transcribe_with_whisper(whisper_client, file_path)\n",
    "display(transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Document Body\n",
      "\n",
      "    Review of Transcription(text=\"Good afternoon and thank you for calling Contoso Suites. This call is being recorded for quality assurance purposes. My name is Cameron Baker. How can I assist you today? Uh, yes, hi. My name is Parker McLean and I'm calling because I wanted to change some of the details on my upcoming stay. Thank you for choosing Contoso Suites, Mr. McLean. Could you please tell me what hotel your reservation is for so I can look up the details of your stay? Uh, I'm staying at the, uh, the Airport Gateway Hotel. Okay, I found your reservation. Can you please confirm your check-in and check-out dates for me? I check in on, uh, on the 9th and check out on... Hang on, let me check. Um, right, right, right, okay. Yeah, so I'm checking in on the 9th and out on the 14th. And I requested two meeting rooms for the 11th and the 12th. Thank you. That matches what I'm seeing on your reservation. What changes were you looking to make today? I'm hoping to change the date from that hotel to the, uh, to the Downtown Plaza Hotel instead. So, um, it'd be a change to my reservation and, and the two meeting rooms. Are you looking to change the dates or will they stay the same? Uh, the dates would be the same. Okay, give me a moment to check if there's availability at the Downtown Plaza Hotel for those dates. Unfortunately, it does not appear the Downtown Plaza Hotel is available for your dates. However, I do have availability for both the room and meeting rooms at the Grand Contoso Downtown Seattle. This hotel is part of Contoso's Luxury Hotels Collection, so it is more expensive, but is only a block away from the Downtown Plaza Hotel, so it will still keep you right in the center of the downtown area. It also features a Michelin Star restaurant, which is one of the best in Seattle. Let me know what the price would be for that switch. The total price for the room, checking in on the 9th and out on the 14th, plus two meeting rooms for the 11th and 12th, comes out to $2,378. That's $1,578 for the room and $800 for the meeting rooms. I also see you're a platinum member of our rewards program. Thank you for your loyalty. For that, I can upgrade you to one of our executive suites. That is a larger room with an excellent view of the Puget Sound. Would you like me to make the change to your upcoming reservation? I'd be grateful. Okay, your reservation has been updated. Thank you. Thank you for choosing Contoso Suites. Have a nice day.\")\n",
      "\n",
      "\n",
      "    |endoftext|\n",
      "\n",
      "    Write answer for questions only referring to the facts in document above. \n",
      "    Sentiment can either be positive, neutral or negative.\n",
      "\n",
      "    |endoftext|\n",
      "\n",
      "    ### Questions\n",
      "    1) What is the overall sentiment?\n",
      "    2) List the sentiment aspects as bullet points.\n",
      "    3) Overall Sentiment\n",
      "    \n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4t - Endpoint: https://admin-m78bg318-swedencentral.openai.azure.com/openai/\n",
      "\n",
      "### Answers\n",
      "\n",
      "1) The overall sentiment is positive.\n",
      "2) List of sentiment aspects:\n",
      "   - The customer service representative was polite and helpful.\n",
      "   - The customer's needs were addressed promptly, even though the initial request could not be fulfilled.\n",
      "   - An alternative option was provided that was close to the desired location and included an upgrade.\n",
      "   - The customer was appreciative of the upgrade and the service provided.\n",
      "3) Overall Sentiment: Positive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"### Answers\\n\\n1) The overall sentiment is positive.\\n2) List of sentiment aspects:\\n   - The customer service representative was polite and helpful.\\n   - The customer's needs were addressed promptly, even though the initial request could not be fulfilled.\\n   - An alternative option was provided that was close to the desired location and included an upgrade.\\n   - The customer was appreciative of the upgrade and the service provided.\\n3) Overall Sentiment: Positive\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_sentiment_aspects_for_batch(transcript, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Document Body\n",
      "\n",
      "    Review of Transcription(text='Hola, acabo de tener un accidente y quería notificarlo. Hola, de acuerdo. Espero que esté bien. ¿Qué ha pasado? Estaba conduciendo por la carretera de Colmenar y me he dado un golpe con otro coche. ¿Está usted bien? Sí, solo un poco nervioso. Es normal. ¿Me puede decir su nombre completo? Claro, me llamo Álvaro Gómez Rodríguez. ¿Sabe cuál ha sido la causa del accidente? Creo que he golpeado un bache. De acuerdo. ¿Dónde se ha producido el accidente? En la carretera de Colmenar, pasada la salida 17. ¿Ha habido algún otro herido? Creo que no, pero no estoy seguro. De acuerdo, lo investigaremos. ¿Me puede dar la información del otro conductor? Sí, su nombre es Juan Delgado Rivera. De acuerdo, un momento, por favor. ¿Me puede decir su DNI, por favor? Sí. Es 12345678F. De acuerdo. ¿Qué daños ha sufrido el coche? Se ha roto el faro delantero derecho y se ha pinchado una rueda. ¿Puede conducir el coche? No lo sé, no. Va a venir a recogerlo la grúa. De acuerdo, necesitaremos inspeccionar el coche. Voy a dar de alta el parte para proceder la petición de inspección y reparación. Perfecto, muchas gracias por su ayuda.')\n",
      "\n",
      "\n",
      "    |endoftext|\n",
      "\n",
      "    Write answer for questions only referring to the facts in document above. \n",
      "    Sentiment can either be positive, neutral or negative.\n",
      "\n",
      "    |endoftext|\n",
      "\n",
      "    ### Questions\n",
      "    1) What is the overall sentiment?\n",
      "    2) List the sentiment aspects as bullet points.\n",
      "    3) Overall Sentiment\n",
      "    \n",
      "\n",
      "Calling OpenAI APIs with 2 messages - Model: gpt-4t - Endpoint: https://admin-m78bg318-swedencentral.openai.azure.com/openai/\n",
      "\n",
      "### Answers\n",
      "\n",
      "1) The overall sentiment is neutral.\n",
      "2) Sentiment aspects:\n",
      "   - The caller is a bit nervous but otherwise unharmed, which could be seen as slightly negative but mostly neutral as the situation is under control.\n",
      "   - The conversation is factual and procedural, focusing on gathering information about the accident and the next steps, which is neutral.\n",
      "   - The assistance provided by the operator is helpful and efficient, which leans towards a positive sentiment.\n",
      "3) Overall Sentiment: Neutral. The conversation mainly revolves around the exchange of necessary information after an accident, without any strong positive or negative emotions displayed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'### Answers\\n\\n1) The overall sentiment is neutral.\\n2) Sentiment aspects:\\n   - The caller is a bit nervous but otherwise unharmed, which could be seen as slightly negative but mostly neutral as the situation is under control.\\n   - The conversation is factual and procedural, focusing on gathering information about the accident and the next steps, which is neutral.\\n   - The assistance provided by the operator is helpful and efficient, which leans towards a positive sentiment.\\n3) Overall Sentiment: Neutral. The conversation mainly revolves around the exchange of necessary information after an accident, without any strong positive or negative emotions displayed.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_sentiment_aspects_for_batch(transcript2, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
